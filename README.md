# Manual TÃ©cnico 
# --- *Procesamiento y Limpieza de Datos para Insight Cluster*

## Participantes

-   **Elder Lopez 201700404**\
-   **Selvin Hernandez 201700603**\
-   **Josue Cux 201700688**

------------------------------------------------------------------------

# InsightCluster â€“ Proyecto 2

AplicaciÃ³n fullâ€‘stack para segmentaciÃ³n de clientes y agrupamiento de reseÃ±as usando K-Means. Incluye limpieza de datos, entrenamiento, evaluaciÃ³n de mÃ©tricas internas y visualizaciÃ³n en frontend.

## DescripciÃ³n General del Proyecto

Esta secciÃ³n realiza un proceso completo de limpieza y depuraciÃ³n de
datos sobre el archivo **`insight_cluster.csv`**, con el objetivo de
dejar un dataset consistente, libre de ruido y listo para la fase de
modelado.

Las etapas cubren: 
- ValidaciÃ³n de columnas clave
- EliminaciÃ³n de registros nulo
- DepuraciÃ³n de valores no numÃ©ricos
- EliminaciÃ³n de duplicados
- ConversiÃ³n de tipos de datos
- DetecciÃ³n y tratamiento de outliers mediante IQR
- ConversiÃ³n y validaciÃ³n de fechas
- ExportaciÃ³n del dataset limpio en `clean_files/data_clean.csv`

------------------------------------------------------------------------

# ðŸ§¹ 1. Proceso de Limpieza de Datos

La limpieza de datos se construye a partir del siguiente archivo fuente:

    insight_cluster.csv

A continuaciÃ³n se documenta tÃ©cnicamente cada etapa aplicada en el
script.

------------------------------------------------------------------------

## 1.1 Carga inicial del dataset

Se establece la ruta del archivo y se cargan los datos utilizando
`pandas`:

-   Se habilita la visualizaciÃ³n completa de columnas.
-   Se imprime un preview (`head`) de los datos cargados.
-   Se muestra el esquema mediante `df.info()` para validar tipos
    iniciales, conteo de nulos y estructura general.

------------------------------------------------------------------------

## 1.2 Manejo de valores faltantes

-   Se eliminan registros donde `cliente_id` sea **nulo**, ya que esta
    columna es considerada identificador Ãºnico y clave primaria del
    cliente.

``` python
df = df.dropna(subset=['cliente_id'])
```

------------------------------------------------------------------------

## 1.3 DetecciÃ³n y eliminaciÃ³n de duplicados

-   Se detectan registros duplicados por `cliente_id`.
-   Se conservan Ãºnicamente los primeros registros, eliminando
    duplicados posteriores.

``` python
duplicados = df[df.duplicated(subset=['cliente_id'], keep=False)]
df = df.drop_duplicates(subset=['cliente_id'], keep='first')
```

------------------------------------------------------------------------

## 1.4 ValidaciÃ³n y limpieza de columnas numÃ©ricas

Las siguientes columnas se consideran numÃ©ricas:

``` python
columns_num = [
    'cliente_id',
    'frecuencia_compra',
    'monto_total_gastado',
    'monto_promedio_compra',
    'dias_desde_ultima_compra',
    'antiguedad_cliente_meses',
    'numero_productos_distintos',
    'reseÃ±a_id'
]
```

Para cada columna se aplica el siguiente pipeline:

### **1. ValidaciÃ³n estricta de valores numÃ©ricos**

-   Se aplica una expresiÃ³n regular para aceptar solo valores numÃ©ricos
    vÃ¡lidos con o sin decimales.
-   **Registros con texto extraÃ±o o formatos invÃ¡lidos (p.Â ej.
    `"4g.26"`) se eliminan por completo.**

``` python
mask_numeric = df[column].astype(str).str.match(r'^\d+(\.\d+)?$')
df = df[mask_numeric]
```

### **2. ConversiÃ³n de tipos**

-   Una vez filtrados, los valores son convertidos a `float`.

``` python
df[column] = df[column].astype(float)
```

### **3. DetecciÃ³n y eliminaciÃ³n de outliers (IQR)**

Se utiliza el rango intercuartÃ­lico (IQR) para eliminar valores
atÃ­picos:

-   CÃ¡lculo de Q1, Q3 e IQR
-   DeterminaciÃ³n de lÃ­mites vÃ¡lidos
-   EliminaciÃ³n de valores fuera del rango permitido

``` python
Q1 = df[column].quantile(0.25)
Q3 = df[column].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR
df = df[(df[column] >= lower) & (df[column] <= upper)]
```

### **4. ImputaciÃ³n**

Los valores faltantes restantes se imputan utilizando **la mediana**.

``` python
df[column] = df[column].fillna(df[column].median())
```

### **5. ConversiÃ³n final a enteros (cuando corresponda)**

Si la columna no contiene valores decimales, se transforma a tipo
`int64`.

------------------------------------------------------------------------

## 1.5 Limpieza de la columna de fecha

La columna `fecha_reseÃ±a` se convierte a tipo fecha con formato
`YYYY-MM-DD`.\
Los registros con fechas invÃ¡lidas se eliminan.

``` python
df['fecha_reseÃ±a'] = pd.to_datetime(df['fecha_reseÃ±a'], format='%Y-%m-%d', errors='coerce')
df = df.dropna(subset=['fecha_reseÃ±a'])
```

------------------------------------------------------------------------

## ðŸ’¾ 1.6 ExportaciÃ³n del dataset final

El dataset limpio se exporta a:

    clean_files/data_clean.csv

Si el archivo existe, se elimina y se genera uno nuevo.

------------------------------------------------------------------------

#  2. ElecciÃ³n del Modelo

## ElecciÃ³n del Modelo: K-Means

**Motivo de elecciÃ³n:**  

El modelo **K-Means** se seleccionÃ³ por su **eficiencia y simplicidad** para agrupar datos sin etiquetas en **clusters homogÃ©neos**. Es ideal cuando se busca **identificar patrones de similitud** y segmentar datos de manera rÃ¡pida.

**Razones especÃ­ficas:**

1. **Simplicidad y rapidez:**  
   K-Means es fÃ¡cil de implementar y computacionalmente eficiente para conjuntos de datos medianos y grandes.

2. **Interpretabilidad:**  
   Los clusters resultantes son fÃ¡ciles de interpretar, ya que cada punto pertenece a un solo cluster y se puede analizar su centroide.

3. **Flexibilidad:**  
   Permite controlar el nÃºmero de clusters (*K*), ajustÃ¡ndose a la necesidad de segmentaciÃ³n del problema.

4. **Efectividad para datos continuos:**  
   Funciona muy bien con datos numÃ©ricos y escalados, como mÃ©tricas de desempeÃ±o, consumo o caracterÃ­sticas medibles de objetos/usuarios.



------------------------------------------------------------------------

#  3. Decisiones del Modelo

## Decisiones del Modelo

Durante el desarrollo del proyecto, se tomaron varias decisiones importantes para garantizar que el modelo K-Means funcionara de manera efectiva y consistente:

1. **NÃºmero de Clusters (*K*):**  
   Se decidiÃ³ determinar el nÃºmero Ã³ptimo de clusters utilizando el **mÃ©todo del codo (Elbow Method)**, evaluando la variaciÃ³n intra-cluster para encontrar un balance entre complejidad y representatividad de los datos.

2. **Escalado de datos:**  
   Todas las variables numÃ©ricas fueron normalizadas usando **StandardScaler** para asegurar que cada caracterÃ­stica contribuyera de manera equitativa al cÃ¡lculo de distancias, evitando que variables con mayor magnitud dominen el clustering.


3. **NÃºmero de iteraciones:**  
   Se configurÃ³ un lÃ­mite mÃ¡ximo de iteraciones para garantizar que el algoritmo convergiera de manera eficiente, evitando ciclos infinitos en casos de datos con alta dispersiÃ³n.

4. **EvaluaciÃ³n de clusters:**  
   Se utilizaron mÃ©tricas internas como **inercia** y **silhouette score** para validar la calidad de los clusters y ajustar parÃ¡metros segÃºn la estructura de los datos.

5. **Tratamiento de valores atÃ­picos:**  
   Se decidiÃ³ analizar y, en ciertos casos, excluir valores atÃ­picos que podÃ­an sesgar los centroides y afectar la segmentaciÃ³n final.

Estas decisiones permiten que el modelo K-Means entregue resultados confiables y Ãºtiles para la interpretaciÃ³n y segmentaciÃ³n de los datos.

Para las reseÃ±as se realizan las elecciones similares, pero aca se aÃ±ade el proceso de NormalizaciÃ³n del texto y la vecotrizaciÃ³n Numerica mediante *TF-IDF*

------------------------------------------------------------------------

#  4. Conclusiones

## Conclusiones

DespuÃ©s de aplicar el modelo K-Means y analizar los resultados, se pueden extraer las siguientes conclusiones:

1. **SegmentaciÃ³n efectiva de los datos:**  
   El modelo logrÃ³ agrupar los datos en clusters coherentes, permitiendo identificar patrones y comportamientos similares dentro de cada grupo.

2. **Importancia de la limpieza y escalado:**  
   La normalizaciÃ³n de los datos y la eliminaciÃ³n de valores atÃ­picos resultaron fundamentales para obtener clusters precisos y representativos.

3. **ValidaciÃ³n de parÃ¡metros:**  
   La elecciÃ³n del nÃºmero de clusters mediante el mÃ©todo del codo y la evaluaciÃ³n con mÃ©tricas como **silhouette score** aseguraron que la segmentaciÃ³n fuera significativa y no arbitraria.

4. **Aplicabilidad prÃ¡ctica:**  
   Los clusters obtenidos pueden utilizarse para **toma de decisiones**

5. **Posibles mejoras:**  
   Futuras iteraciones podrÃ­an incluir la comparaciÃ³n con otros algoritmos de clustering (por ejemplo, DBSCAN o Gaussian Mixture Models) y la incorporaciÃ³n de mÃ¡s variables para aumentar la riqueza del anÃ¡lisis.

En general, el modelo K-Means demostrÃ³ ser **una herramienta eficiente y confiable** para el anÃ¡lisis no supervisado de este conjunto de datos.


------------------------------------------------------------------------

#  5. Decisiones Tomadas Durante el Desarrollo

## Decisiones Tomadas Durante el Desarrollo

Durante el desarrollo del proyecto se tomaron decisiones clave para asegurar la correcta preparaciÃ³n de los datos y la efectividad del modelo K-Means:

1. **SelecciÃ³n de variables relevantes:**  
   Se analizaron todas las variables disponibles y se eligieron Ãºnicamente aquellas que aportan informaciÃ³n significativa para el clustering, eliminando columnas redundantes o con muchos valores faltantes.

2. **Tratamiento de datos faltantes:**  
   Se optÃ³ por imputar o eliminar registros segÃºn el contexto de la variable, garantizando que los datos utilizados fueran consistentes y no afectaran la segmentaciÃ³n.

3. **NormalizaciÃ³n y escalado:**  
   Todas las caracterÃ­sticas numÃ©ricas fueron normalizadas para asegurar que cada variable contribuyera equitativamente al cÃ¡lculo de distancias entre los datos.

4. **ElecciÃ³n de la tÃ©cnica de clustering:**  
   Se seleccionÃ³ **K-Means** por su simplicidad, rapidez y facilidad de interpretaciÃ³n, siendo adecuado para conjuntos de datos medianos y continuos.

5. **DeterminaciÃ³n del nÃºmero de clusters (*K*):**  
   Se utilizÃ³ el **mÃ©todo del codo (Elbow Method)** para encontrar un balance entre la compactaciÃ³n de los clusters y la complejidad del modelo.


6. **Manejo de valores atÃ­picos:**  
   Se identificaron y trataron valores atÃ­picos que podrÃ­an distorsionar los centroides y afectar la interpretaciÃ³n de los clusters.

Estas decisiones fueron fundamentales para **asegurar la confiabilidad del modelo** y obtener resultados que puedan ser utilizados para anÃ¡lisis y toma de decisiones posteriores.




---

## Arquitectura
- **Backend:** Flask (Python), endpoints REST en `Back/app/routes/`, lÃ³gica en `Back/app/services/`, respuestas uniformes con `Back/app/utils/response.py`.
- **Modelo:** K-Means (scikit-learn) para dos vistas: clientes (datos numÃ©ricos + categorÃ­a) y reseÃ±as (texto con TF-IDF).
- **Frontend:** React + Vite + TypeScript (`front-insightcluster/`), rutas con React Router.

## Flujo de trabajo (end-to-end)
1) **Carga**: Subir CSV en la vista Carga Masiva (`/`) â†’ POST `/api/file/upload`.
2) **Limpieza**: Ejecutar limpieza â†’ GET `/api/file/limpieza` â†’ genera `Back/clean_files/data_clean.csv`.
3) **Ajuste**: Configurar hiperparÃ¡metros en Ajuste (`/ajuste`) â†’ POST `/api/model/set_stats` (clientes y reseÃ±as por separado).
4) **Entrenamiento**: Se ejecuta K-Means para clientes y reseÃ±as; guarda artefactos en `Back/models_ml/`.
5) **EvaluaciÃ³n**: Consultar mÃ©tricas en EvaluaciÃ³n (`/evaluacion`) â†’ GET `/api/model/stats`.

## Endpoints principales
- POST `/api/file/upload` â€“ sube CSV crudo.
- GET `/api/file/limpieza` â€“ limpia y escribe `clean_files/data_clean.csv`.
- GET `/api/model/training` â€“ entrenamiento con valores por defecto (K=5, RS=42, max_iter=20 en cÃ³digo base).
- POST `/api/model/set_stats` â€“ entrenamiento con hiperparÃ¡metros enviados desde el frontend.
- GET `/api/model/stats` â€“ expone mÃ©tricas internas de clustering para clientes y reseÃ±as.

## Modelo y preprocesamiento
**Clientes**
- Features: frecuencia_compra, monto_total_gastado, monto_promedio_compra, dias_desde_ultima_compra, antiguedad_cliente_meses, numero_productos_distintos, canal_principal.
- Preproceso: one-hot a canal_principal, escalado con StandardScaler.
- Modelo: K-Means (n_clusters=K, random_state=RS, max_iter=MI, n_init=20).
- Artefactos: columnas, scaler, modelo guardados en `models_ml/clientes/`.

**ReseÃ±as**
- Texto en `texto_reseÃ±a`.
- Preproceso: limpieza bÃ¡sica (lower, sin nÃºmeros/puntuaciÃ³n), TF-IDF (max_features=500, min_df=2, max_df=0.9, ngram_range=(1,2), stopwords ES).
- Modelo: K-Means con los mismos hiperparÃ¡metros.
- Artefactos: vectorizador y modelo en `models_ml/reseÃ±as/`.
- DescripciÃ³n de temas: top palabras de cada centroide + mapeo a aspectos (usabilidad, rendimiento, experiencia, cumplimiento, soporte).
## Estructura del backend
```
Back/
â”œâ”€â”€ app.py                  # Punto de entrada Flask
â”œâ”€â”€ requirements.txt        # Dependencias backend
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ file_routes.py      # Upload y limpieza
â”‚   â”‚   â””â”€â”€ version_routes.py   # InformaciÃ³n de versiÃ³n/health
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ file_service.py     # LÃ³gica de carga/limpieza
â”‚   â”‚   â””â”€â”€ version_service.py  # LÃ³gica de versiÃ³n
â”‚   â””â”€â”€ utils/response.py       # Respuestas estÃ¡ndar
â”œâ”€â”€ clean_files/
â”‚   â””â”€â”€ data_clean.csv          # Dataset limpio generado
â”œâ”€â”€ models_ml/                  # Modelos y artefactos
â””â”€â”€ uploads/                    # Archivos subidos
```

## Estructura del frontend
```
front-insightcluster/
â”œâ”€â”€ index.html
â”œâ”€â”€ vite.config.ts
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig*.json
â”œâ”€â”€ public/
â””â”€â”€ src/
    â”œâ”€â”€ main.tsx               # Entrypoint React
    â”œâ”€â”€ App.tsx                # Rutas principales
    â”œâ”€â”€ constant/url.ts        # Base URL API
    â”œâ”€â”€ pages/
    â”‚   â”œâ”€â”€ CargaMasiva.tsx    # Subida y limpieza
    â”‚   â”œâ”€â”€ Ajuste.tsx         # Ajuste de KMeans
    â”‚   â””â”€â”€ Evaluacion.tsx     # MÃ©tricas de clustering
    â”œâ”€â”€ components/
    â”‚   â”œâ”€â”€ Navbar.tsx
    â”‚   â”œâ”€â”€ UploadBox.tsx
    â”‚   â””â”€â”€ ConfigPanel.tsx
    â””â”€â”€ styles/                # CSS modular (ajuste, evaluaciÃ³n, etc.)
```

## Licencia
Proyecto acadÃ©mico; uso interno para la entrega del curso.
- `models/modelo_entrenado.pkl`: Modelo entrenado y mÃ©tricas.

---

## 7. Problemas Comunes y SoluciÃ³n

- "No hay datos cargados": Cargar CSV vÃ­a `/api/cargaMasiva` antes de limpiar/entrenar.
- "Modelo no disponible": Ejecutar entrenamiento para generar `models/modelo_entrenado.pkl`.
- Error en predicciÃ³n por campos faltantes: completar todos los requeridos del endpoint.
- MÃ©tricas no disponibles: entrenar y luego consultar `/api/Rendimiento/consultar_metricas`.

### 7.1 Dependencias del Backend

Se definiÃ³ las siguientes dependencias en `requirements.txt`:

```
Flask==3.1.2          # Framework web
flask-cors==6.0.1     # Manejo de CORS
matplotlib==3.10.7    # VisualizaciÃ³n de datos
numpy==2.3.5          # Operaciones numÃ©ricas
pandas==2.3.3         # ManipulaciÃ³n de datos
scikit-learn==1.7.2   # Machine Learning
```

---

## 8. Estructura del Frontend

El desarrollador implementÃ³ el frontend con la siguiente organizaciÃ³n:

### 8.1 Estructura de Directorios

```
frontend/
â”œâ”€â”€ index.html              # Punto de entrada HTML
â”œâ”€â”€ package.json            # ConfiguraciÃ³n y dependencias
â”œâ”€â”€ vite.config.js         # ConfiguraciÃ³n de Vite
â”œâ”€â”€ public/                # Recursos estÃ¡ticos
â””â”€â”€ src/                   # CÃ³digo fuente
    â”œâ”€â”€ main.jsx           # Entrada de React
    â”œâ”€â”€ App.jsx            # Componente principal
    â”œâ”€â”€ App.css            # Estilos globales
    â”œâ”€â”€ index.css          # Estilos base
    â”œâ”€â”€ style.css          # Estilos adicionales
    â”œâ”€â”€ components/        # Componentes reutilizables
    â”‚   â”œâ”€â”€ CargaArchivo.jsx    
    â”‚   â”œâ”€â”€ Limpieza.jsx
    â”‚   â”œâ”€â”€ Metricas.jsx
    â”‚   â”œâ”€â”€ ModeloE.jsx
    â”‚   â””â”€â”€ Prediccion.jsx
    â”œâ”€â”€ pages/             # PÃ¡ginas principales
    â”‚   â”œâ”€â”€ Dashboard.jsx
    â”‚   â”œâ”€â”€ Dashboard.module.css
    â”‚   â””â”€â”€ Home.jsx
    â”œâ”€â”€ constant/          # Constantes de configuraciÃ³n
    â”‚   â””â”€â”€ url.js
    â””â”€â”€ hooks/             # React Hooks personalizados
```