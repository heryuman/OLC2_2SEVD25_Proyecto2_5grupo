# Manual T茅cnico 
# --- *Procesamiento y Limpieza de Datos para Insight Cluster*

## Participantes

-   **Elder Lopez 201700404**\
-   **Selvin Hernandez 201700603**\
-   **Josue Cux 201700688**

------------------------------------------------------------------------

## Descripci贸n General del Proyecto

Esta secci贸n realiza un proceso completo de limpieza y depuraci贸n de
datos sobre el archivo **`insight_cluster.csv`**, con el objetivo de
dejar un dataset consistente, libre de ruido y listo para la fase de
modelado.

Las etapas cubren: 
- Validaci贸n de columnas clave
- Eliminaci贸n de registros nulo
- Depuraci贸n de valores no num茅ricos
- Eliminaci贸n de duplicados
- Conversi贸n de tipos de datos
- Detecci贸n y tratamiento de outliers mediante IQR
- Conversi贸n y validaci贸n de fechas
- Exportaci贸n del dataset limpio en `clean_files/data_clean.csv`

------------------------------------------------------------------------

# Ч 1. Proceso de Limpieza de Datos

La limpieza de datos se construye a partir del siguiente archivo fuente:

    insight_cluster.csv

A continuaci贸n se documenta t茅cnicamente cada etapa aplicada en el
script.

------------------------------------------------------------------------

## 1.1 Carga inicial del dataset

Se establece la ruta del archivo y se cargan los datos utilizando
`pandas`:

-   Se habilita la visualizaci贸n completa de columnas.
-   Se imprime un preview (`head`) de los datos cargados.
-   Se muestra el esquema mediante `df.info()` para validar tipos
    iniciales, conteo de nulos y estructura general.

------------------------------------------------------------------------

## 1.2 Manejo de valores faltantes

-   Se eliminan registros donde `cliente_id` sea **nulo**, ya que esta
    columna es considerada identificador 煤nico y clave primaria del
    cliente.

``` python
df = df.dropna(subset=['cliente_id'])
```

------------------------------------------------------------------------

## 1.3 Detecci贸n y eliminaci贸n de duplicados

-   Se detectan registros duplicados por `cliente_id`.
-   Se conservan 煤nicamente los primeros registros, eliminando
    duplicados posteriores.

``` python
duplicados = df[df.duplicated(subset=['cliente_id'], keep=False)]
df = df.drop_duplicates(subset=['cliente_id'], keep='first')
```

------------------------------------------------------------------------

## 1.4 Validaci贸n y limpieza de columnas num茅ricas

Las siguientes columnas se consideran num茅ricas:

``` python
columns_num = [
    'cliente_id',
    'frecuencia_compra',
    'monto_total_gastado',
    'monto_promedio_compra',
    'dias_desde_ultima_compra',
    'antiguedad_cliente_meses',
    'numero_productos_distintos',
    'rese帽a_id'
]
```

Para cada columna se aplica el siguiente pipeline:

### **1. Validaci贸n estricta de valores num茅ricos**

-   Se aplica una expresi贸n regular para aceptar solo valores num茅ricos
    v谩lidos con o sin decimales.
-   **Registros con texto extra帽o o formatos inv谩lidos (p.ej.
    `"4g.26"`) se eliminan por completo.**

``` python
mask_numeric = df[column].astype(str).str.match(r'^\d+(\.\d+)?$')
df = df[mask_numeric]
```

### **2. Conversi贸n de tipos**

-   Una vez filtrados, los valores son convertidos a `float`.

``` python
df[column] = df[column].astype(float)
```

### **3. Detecci贸n y eliminaci贸n de outliers (IQR)**

Se utiliza el rango intercuart铆lico (IQR) para eliminar valores
at铆picos:

-   C谩lculo de Q1, Q3 e IQR
-   Determinaci贸n de l铆mites v谩lidos
-   Eliminaci贸n de valores fuera del rango permitido

``` python
Q1 = df[column].quantile(0.25)
Q3 = df[column].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR
df = df[(df[column] >= lower) & (df[column] <= upper)]
```

### **4. Imputaci贸n**

Los valores faltantes restantes se imputan utilizando **la mediana**.

``` python
df[column] = df[column].fillna(df[column].median())
```

### **5. Conversi贸n final a enteros (cuando corresponda)**

Si la columna no contiene valores decimales, se transforma a tipo
`int64`.

------------------------------------------------------------------------

## 1.5 Limpieza de la columna de fecha

La columna `fecha_rese帽a` se convierte a tipo fecha con formato
`YYYY-MM-DD`.\
Los registros con fechas inv谩lidas se eliminan.

``` python
df['fecha_rese帽a'] = pd.to_datetime(df['fecha_rese帽a'], format='%Y-%m-%d', errors='coerce')
df = df.dropna(subset=['fecha_rese帽a'])
```

------------------------------------------------------------------------

##  1.6 Exportaci贸n del dataset final

El dataset limpio se exporta a:

    clean_files/data_clean.csv

Si el archivo existe, se elimina y se genera uno nuevo.

------------------------------------------------------------------------

#  2. Elecci贸n del Modelo

## Elecci贸n del Modelo: K-Means

**Motivo de elecci贸n:**  

El modelo **K-Means** se seleccion贸 por su **eficiencia y simplicidad** para agrupar datos sin etiquetas en **clusters homog茅neos**. Es ideal cuando se busca **identificar patrones de similitud** y segmentar datos de manera r谩pida.

**Razones espec铆ficas:**

1. **Simplicidad y rapidez:**  
   K-Means es f谩cil de implementar y computacionalmente eficiente para conjuntos de datos medianos y grandes.

2. **Interpretabilidad:**  
   Los clusters resultantes son f谩ciles de interpretar, ya que cada punto pertenece a un solo cluster y se puede analizar su centroide.

3. **Flexibilidad:**  
   Permite controlar el n煤mero de clusters (*K*), ajust谩ndose a la necesidad de segmentaci贸n del problema.

4. **Efectividad para datos continuos:**  
   Funciona muy bien con datos num茅ricos y escalados, como m茅tricas de desempe帽o, consumo o caracter铆sticas medibles de objetos/usuarios.



------------------------------------------------------------------------

#  3. Decisiones del Modelo (Pendiente)

## Decisiones del Modelo

Durante el desarrollo del proyecto, se tomaron varias decisiones importantes para garantizar que el modelo K-Means funcionara de manera efectiva y consistente:

1. **N煤mero de Clusters (*K*):**  
   Se decidi贸 determinar el n煤mero 贸ptimo de clusters utilizando el **m茅todo del codo (Elbow Method)**, evaluando la variaci贸n intra-cluster para encontrar un balance entre complejidad y representatividad de los datos.

2. **Escalado de datos:**  
   Todas las variables num茅ricas fueron normalizadas usando **StandardScaler** para asegurar que cada caracter铆stica contribuyera de manera equitativa al c谩lculo de distancias, evitando que variables con mayor magnitud dominen el clustering.


3. **N煤mero de iteraciones:**  
   Se configur贸 un l铆mite m谩ximo de iteraciones para garantizar que el algoritmo convergiera de manera eficiente, evitando ciclos infinitos en casos de datos con alta dispersi贸n.

4. **Evaluaci贸n de clusters:**  
   Se utilizaron m茅tricas internas como **inercia** y **silhouette score** para validar la calidad de los clusters y ajustar par谩metros seg煤n la estructura de los datos.

5. **Tratamiento de valores at铆picos:**  
   Se decidi贸 analizar y, en ciertos casos, excluir valores at铆picos que pod铆an sesgar los centroides y afectar la segmentaci贸n final.

Estas decisiones permiten que el modelo K-Means entregue resultados confiables y 煤tiles para la interpretaci贸n y segmentaci贸n de los datos.


------------------------------------------------------------------------

#  4. Conclusiones

## Conclusiones

Despu茅s de aplicar el modelo K-Means y analizar los resultados, se pueden extraer las siguientes conclusiones:

1. **Segmentaci贸n efectiva de los datos:**  
   El modelo logr贸 agrupar los datos en clusters coherentes, permitiendo identificar patrones y comportamientos similares dentro de cada grupo.

2. **Importancia de la limpieza y escalado:**  
   La normalizaci贸n de los datos y la eliminaci贸n de valores at铆picos resultaron fundamentales para obtener clusters precisos y representativos.

3. **Validaci贸n de par谩metros:**  
   La elecci贸n del n煤mero de clusters mediante el m茅todo del codo y la evaluaci贸n con m茅tricas como **silhouette score** aseguraron que la segmentaci贸n fuera significativa y no arbitraria.

4. **Aplicabilidad pr谩ctica:**  
   Los clusters obtenidos pueden utilizarse para **toma de decisiones**

5. **Posibles mejoras:**  
   Futuras iteraciones podr铆an incluir la comparaci贸n con otros algoritmos de clustering (por ejemplo, DBSCAN o Gaussian Mixture Models) y la incorporaci贸n de m谩s variables para aumentar la riqueza del an谩lisis.

En general, el modelo K-Means demostr贸 ser **una herramienta eficiente y confiable** para el an谩lisis no supervisado de este conjunto de datos.


------------------------------------------------------------------------

#  5. Decisiones Tomadas Durante el Desarrollo

## Decisiones Tomadas Durante el Desarrollo

Durante el desarrollo del proyecto se tomaron decisiones clave para asegurar la correcta preparaci贸n de los datos y la efectividad del modelo K-Means:

1. **Selecci贸n de variables relevantes:**  
   Se analizaron todas las variables disponibles y se eligieron 煤nicamente aquellas que aportan informaci贸n significativa para el clustering, eliminando columnas redundantes o con muchos valores faltantes.

2. **Tratamiento de datos faltantes:**  
   Se opt贸 por imputar o eliminar registros seg煤n el contexto de la variable, garantizando que los datos utilizados fueran consistentes y no afectaran la segmentaci贸n.

3. **Normalizaci贸n y escalado:**  
   Todas las caracter铆sticas num茅ricas fueron normalizadas para asegurar que cada variable contribuyera equitativamente al c谩lculo de distancias entre los datos.

4. **Elecci贸n de la t茅cnica de clustering:**  
   Se seleccion贸 **K-Means** por su simplicidad, rapidez y facilidad de interpretaci贸n, siendo adecuado para conjuntos de datos medianos y continuos.

5. **Determinaci贸n del n煤mero de clusters (*K*):**  
   Se utiliz贸 el **m茅todo del codo (Elbow Method)** para encontrar un balance entre la compactaci贸n de los clusters y la complejidad del modelo.


6. **Manejo de valores at铆picos:**  
   Se identificaron y trataron valores at铆picos que podr铆an distorsionar los centroides y afectar la interpretaci贸n de los clusters.

Estas decisiones fueron fundamentales para **asegurar la confiabilidad del modelo** y obtener resultados que puedan ser utilizados para an谩lisis y toma de decisiones posteriores.

